{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('all')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lUf8Vh_OW3T",
        "outputId": "a47d27c6-a4da-477c-bb0d-d803a8bd75dd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw=set(stopwords.words('english'))\n",
        "print(\"Stopwords are /n:\", sw)\n",
        "\n",
        "#lower case convert\n",
        "text=\"     Welcome to Compiler construction lab 55 and 6 course. Here you can learn NLP.\"\n",
        "store=text.lower()\n",
        "print(\"Text lower:\",store)\n",
        "\n",
        "#remove any number\n",
        "remove= re.sub(r'\\d+','',store)\n",
        "print(\"Remove digits: \",remove)\n",
        "\n",
        "#remove any punctuation(coma, fullstop)\n",
        "#[]=Square brackets define a character set,^\tInside [],\n",
        "# ^ means \"not\" (negation)\n",
        "#\\w\tMatches any word character → letters, digits, and underscore (a-z, A-Z, 0-9, _)\n",
        "#\\s\tMatches any whitespace character → spaces, tabs, newlines\n",
        "#[^\\w\\s]\tMatches any character that is NOT a word character or whitespace\n",
        "removepunc=re.sub(r'[^\\w\\s]','',remove)\n",
        "print(\"Remove punctuation is: \",removepunc)\n",
        "\n",
        "#remove whitespace\n",
        "removewhspace=removepunc.strip()\n",
        "print(\"Remove whitespace:\",removewhspace)\n",
        "\n",
        "#remove stopwords\n",
        "swbag=\"\"\n",
        "lst=removewhspace.split() #convert into list\n",
        "print(\"list of words:\",lst)\n",
        "\n",
        "for i in lst:\n",
        "  if not i in sw:\n",
        "    swbag+=i + ' '\n",
        "print(\"Without stopwords: \",swbag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4K2tIe2w4Ek",
        "outputId": "58a71ad2-564a-49f3-8fd7-ad0a30552f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords are /n: {'ourselves', 'doesn', 'who', 'all', 'is', 'own', 'than', 'my', 'both', 'under', 'once', 'same', 'very', 'am', \"shouldn't\", \"i'd\", 'don', \"they've\", 'where', \"we'll\", 'more', \"wouldn't\", 'been', 'you', \"she'd\", 'yours', 'to', 'whom', 'won', \"hasn't\", 'in', 'now', 'she', 'having', 'because', \"it's\", 'this', 'further', 't', 'myself', \"i've\", 'hadn', 'himself', 'that', \"i'm\", 'itself', 'll', 'not', 'just', 'hers', \"it'll\", 'which', \"they're\", 'me', 'we', \"we're\", 'most', 'on', 'at', 'some', 'will', 'can', 're', 'isn', 'between', 'themselves', 'couldn', 'yourself', 'off', \"you're\", \"doesn't\", 'here', \"you've\", 'these', \"they'll\", 'there', 'out', 'the', \"he'll\", \"haven't\", 'or', \"that'll\", 'o', 'from', 'have', 'mightn', 'ain', 'against', 'be', 'his', \"they'd\", 'its', 'nor', 'so', 'wouldn', 'up', 'an', 'below', \"she's\", 'too', 'few', 'were', 'about', 'your', \"we've\", 'shan', 'they', 'should', 'during', 'shouldn', \"aren't\", 'for', \"we'd\", 'them', 'a', 'do', 'theirs', 'aren', 'then', 'was', \"he's\", 'ma', 'while', 'until', \"don't\", 'down', \"you'll\", \"hadn't\", 'haven', 'yourselves', 'over', \"mightn't\", 'with', 'what', \"wasn't\", \"weren't\", 'above', 'other', 'are', 'herself', 'but', 'why', 'ours', 'through', 'wasn', 'how', 'being', 'had', 'mustn', 'has', 'before', 'again', 'when', \"i'll\", \"isn't\", 'no', \"needn't\", 'does', 'doing', 'he', 'him', 'after', 'as', 'each', \"you'd\", 'y', 'didn', \"should've\", 've', 'our', \"won't\", \"couldn't\", 'd', 'hasn', 'into', 'm', 'and', 'their', \"he'd\", 'did', 'of', 'only', 'those', 'needn', \"it'd\", \"she'll\", \"didn't\", 'by', 'if', 'such', 'it', \"mustn't\", \"shan't\", 'her', 's', 'i', 'any', 'weren'}\n",
            "Text lower:      welcome to compiler construction lab 55 and 6 course. here you can learn nlp.\n",
            "Remove digits:       welcome to compiler construction lab  and  course. here you can learn nlp.\n",
            "Remove punctuation is:       welcome to compiler construction lab  and  course here you can learn nlp\n",
            "Remove whitespace: welcome to compiler construction lab  and  course here you can learn nlp\n",
            "list of words: ['welcome', 'to', 'compiler', 'construction', 'lab', 'and', 'course', 'here', 'you', 'can', 'learn', 'nlp']\n",
            "Without stopwords:  welcome compiler construction lab course learn nlp \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3xERFtBy6Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "sent=\"Welcome to Compiler construction lab course.\\\n",
        " Here you can learn NLP\"\n",
        "print(\"Word tokenize: \",word_tokenize(sent))\n",
        "print(\"Word are: \",len(word_tokenize(sent))) #lab-2\n",
        "print(\"Sentence tokenize: \",sent_tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl_bYo2_qho9",
        "outputId": "276832bd-5dbc-4600-ce44-20c48fdf70af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenize:  ['Welcome', 'to', 'Compiler', 'construction', 'lab', 'course', '.', 'Here', 'you', 'can', 'learn', 'NLP']\n",
            "Word are:  12\n",
            "Sentence tokenize:  ['Welcome to Compiler construction lab course.', 'Here you can learn NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "porter1=WordNetLemmatizer()\n",
        "print(porter.stem(\"eat\"))\n",
        "print(porter.stem(\"eating\"))\n",
        "print(porter.stem(\"eats\"))\n",
        "print(\"ATE:\", porter1.lemmatize(\"ate\",'v'))\n",
        "print(porter1.lemmatize(\"better\",'v'))\n",
        "# create an object of class PorterStemmer\n",
        "\n",
        "print(porter.stem(\"Pronunciation\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WgT52vp1Pr6",
        "outputId": "ac1d1c3f-9f85-4899-a602-592ba3101190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n",
            "eat\n",
            "ATE: eat\n",
            "better\n",
            "pronunci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "word=WordNetLemmatizer()\n",
        "print(word.lemmatize(\"went\",'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsxsr8Q18LzC",
        "outputId": "7c59476c-a7f4-4f73-f64a-6740a9ae8bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "word1=\"Welcome to Compiler construction lab course.\"\n",
        "token=word_tokenize(word1)\n",
        "print(\"Tokenize: \",token)\n",
        "posa=pos_tag(token)\n",
        "print(posa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGLvRGWFBVxB",
        "outputId": "9dfd037a-eff9-4259-ab46-75c955b757d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize:  ['Welcome', 'to', 'Compiler', 'construction', 'lab', 'course', '.']\n",
            "[('Welcome', 'VB'), ('to', 'TO'), ('Compiler', 'NNP'), ('construction', 'NN'), ('lab', 'NN'), ('course', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP_lab-2**"
      ],
      "metadata": {
        "id": "hbwW5zSkD3MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP lab-2_Extracting Email, block letters and numbers"
      ],
      "metadata": {
        "id": "igJEA9C0I2k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text=\"this is JAVA lab 1 and the GMAIL address of java LAB is java802@gmail.com and compiler802@gmail.com\"\n",
        "result=re.findall('\\S+@\\S+', text)\n",
        "print(\"Emails are: \",result)\n",
        "block=re.findall('[a-z,A-Z]+',text)\n",
        "print(\"Blocks are: \",block)\n",
        "number=re.findall('[0-9]+',text)\n",
        "print(\"Numbers are: \",number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1LNNDmIxb9",
        "outputId": "5aa278e4-4dd2-46c4-91aa-5c2b22f803bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails are:  ['java802@gmail.com', 'compiler802@gmail.com']\n",
            "Blocks are:  ['this', 'is', 'JAVA', 'lab', 'and', 'the', 'GMAIL', 'address', 'of', 'java', 'LAB', 'is', 'java', 'gmail', 'com', 'and', 'compiler', 'gmail', 'com']\n",
            "Numbers are:  ['1', '802', '802']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token generation and count (FREQUENCY count for word in English)"
      ],
      "metadata": {
        "id": "0MgFP63gJ6Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict, defaultdict\n",
        "# OrderedDict:  store tokens in the exact order they appear in text.\n",
        "# defaultdict: count word frequencies easily without if key in dict checks.\n",
        "\n",
        "def token(text):\n",
        "  low=text.lower()\n",
        "  print(\"lower case: \",low)\n",
        "  #find word and punctuation\n",
        "  tok=re.findall(r'\\w+|[^\\w\\s]',low)\n",
        "  print(\"Tok\",tok)\n",
        " # Count frequencies using defaultdict\n",
        "  freq = defaultdict(int)\n",
        "  for t in tok:\n",
        "        freq[t] += 1\n",
        "\n",
        "    # Keep the order of first appearance\n",
        "  ordered_freq = OrderedDict()\n",
        "  for t in tok:\n",
        "        if t not in ordered_freq:\n",
        "            ordered_freq[t] = freq[t]\n",
        "\n",
        "  return tok, ordered_freq\n",
        "\n",
        "text= \"Welcome to Compiler construction lab course; Here you can learn NLP through compiler construction.\"\n",
        "word, frequency=token(text)\n",
        "cou=len(word)\n",
        "print(f\"There are {cou} tokens in this line\" )\n",
        "print(\"Frequency of each tokens are:\")\n",
        "for key, value in frequency.items():\n",
        "  print(f\" {key:} {value}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRUwmJvh-AGp",
        "outputId": "f9ad0818-f22d-4cf3-f3f5-f54567db8188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower case:  welcome to compiler construction lab course; here you can learn nlp through compiler construction.\n",
            "Tok ['welcome', 'to', 'compiler', 'construction', 'lab', 'course', ';', 'here', 'you', 'can', 'learn', 'nlp', 'through', 'compiler', 'construction', '.']\n",
            "There are 16 tokens in this line\n",
            "Frequency of each tokens are:\n",
            " welcome 1\n",
            " to 1\n",
            " compiler 2\n",
            " construction 2\n",
            " lab 1\n",
            " course 1\n",
            " ; 1\n",
            " here 1\n",
            " you 1\n",
            " can 1\n",
            " learn 1\n",
            " nlp 1\n",
            " through 1\n",
            " . 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FREQUENCY count for word in English"
      ],
      "metadata": {
        "id": "8ZTpDL2D0wal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "sent=\"Welcome to Compiler construction lab course.\\\n",
        " Here you can learn NLP through Compiler construction.\"\n",
        "print(\"Sentence tokenize: \",sent_tokenize(sent))\n",
        "token= word_tokenize(sent)\n",
        "print(\"Word tokenize: \",token)\n",
        "print(\"Total tokens are: \",len(word_tokenize(sent)))\n",
        "#mentioning Freqdist class\n",
        "freq_dist = FreqDist(token)\n",
        "\n",
        "#Print each word with its frequency\n",
        "for key, value in freq_dist.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjy8aai8WMcz",
        "outputId": "b1a9e4c8-331f-4024-ef62-433bd01f2f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokenize:  ['Welcome to Compiler construction lab course.', 'Here you can learn NLP through Compiler construction.']\n",
            "Word tokenize:  ['Welcome', 'to', 'Compiler', 'construction', 'lab', 'course', '.', 'Here', 'you', 'can', 'learn', 'NLP', 'through', 'Compiler', 'construction', '.']\n",
            "Total tokens are:  16\n",
            "Welcome: 1\n",
            "to: 1\n",
            "Compiler: 2\n",
            "construction: 2\n",
            "lab: 1\n",
            "course: 1\n",
            ".: 2\n",
            "Here: 1\n",
            "you: 1\n",
            "can: 1\n",
            "learn: 1\n",
            "NLP: 1\n",
            "through: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nltk  library import for bengali word and sentence tokenization"
      ],
      "metadata": {
        "id": "E8oTBjpH0mMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # pretarined tokenizer model\n",
        "# punkt = pre-trained tokenization model in NLTK for splitting text into sentences and words accurately.\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"আমি বাংলায় লিখছি । তুমি কি আসছ?\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "word1=word_tokenize(text)\n",
        "print(word1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g37mQSb9Z0e5",
        "outputId": "8e4760f2-7fea-45ec-c843-fb14a038bfe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['আমি বাংলায় লিখছি । তুমি কি আসছ?']\n",
            "['আমি', 'বাংলায়', 'লিখছি', '।', 'তুমি', 'কি', 'আসছ', '?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Bangla library\n",
        "!pip install bnltk\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eRcNlqKriO-v",
        "outputId": "f8fe8339-4f50-462e-96f1-f21c8479069b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bnltk in /usr/local/lib/python3.11/dist-packages (0.7.8)\n",
            "Requirement already satisfied: black==24.10.0 in /usr/local/lib/python3.11/dist-packages (from bnltk) (24.10.0)\n",
            "Requirement already satisfied: keras==3.6.0 in /usr/local/lib/python3.11/dist-packages (from bnltk) (3.6.0)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.11/dist-packages (from bnltk) (2.0.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from bnltk) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (from bnltk) (1.5.2)\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (from bnltk) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black==24.10.0->bnltk) (8.2.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black==24.10.0->bnltk) (1.1.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from black==24.10.0->bnltk) (25.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black==24.10.0->bnltk) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black==24.10.0->bnltk) (4.3.8)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras==3.6.0->bnltk) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras==3.6.0->bnltk) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras==3.6.0->bnltk) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras==3.6.0->bnltk) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras==3.6.0->bnltk) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras==3.6.0->bnltk) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->bnltk) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->bnltk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->bnltk) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->bnltk) (2025.8.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2->bnltk) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2->bnltk) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2->bnltk) (3.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->bnltk) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0->bnltk) (0.45.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->bnltk) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->bnltk) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->bnltk) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.6.0->bnltk) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.6.0->bnltk) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.6.0->bnltk) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0->bnltk) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bengali bnltk library import for word tokenization"
      ],
      "metadata": {
        "id": "aERGDOlj0gWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Tokenizers class from bnltk.tokenize\n",
        "from bnltk.tokenize import Tokenizers\n",
        "#from bnltk.tokenize import bn_sentence_tokenizer\n",
        "\n",
        "# Initialize the Tokenizers object\n",
        "#tokenizer = Assigns the newly created object to the variable tokenizer.\n",
        "tokenizer = Tokenizers()\n",
        "\n",
        "# Bengali text to tokenize\n",
        "text = \"আমি স্কুলে যাচ্ছি। সে বইটি পড়ছে। আজকের দিনটা সুন্দর। \"\n",
        "text1= \"সে বইটি পড়ছে। আমরা বন্ধুরা খেলতে যাচ্ছি।\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.bn_word_tokenizer(text)\n",
        "tokens1 = tokenizer.bn_word_tokenizer(text1)\n",
        "#sentences = tokenizer.bn_sentence_tokenizer(text)\n",
        "# Print the tokens\n",
        "print(tokens)\n",
        "#print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmfHV5xji3YA",
        "outputId": "c3f35572-90ee-4054-8655-5e71ac8714b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['আমি', 'স্কুলে', 'যাচ্ছি', '।', 'সে', 'বইটি', 'পড়ছে', '।', 'আজকের', 'দিনটা', 'সুন্দর', '।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install indic-nlp-library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RA3C8tStksYM",
        "outputId": "b631467d-9ce9-421b-982d-edd7e12392d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.11/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bengali indiacnlp library import for word and sentence tokenization"
      ],
      "metadata": {
        "id": "4V7D-vCZ0Z1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "text = \"আমি স্কুলে যাচ্ছি। সে বইটি পড়ছে। আজকের দিনটা সুন্দর। \"\n",
        "tokens = list(indic_tokenize.trivial_tokenize(text))\n",
        "print(tokens)\n",
        "sentences = sentence_tokenize.sentence_split(text, lang='bn')\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkUvgPyyk4vN",
        "outputId": "e135905b-316e-4020-a9c0-79159ae724a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['আমি', 'স্কুলে', 'যাচ্ছি', '।', 'সে', 'বইটি', 'পড়ছে', '।', 'আজকের', 'দিনটা', 'সুন্দর', '।']\n",
            "['আমি স্কুলে যাচ্ছি।', 'সে বইটি পড়ছে।', 'আজকের দিনটা সুন্দর।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bengali frequency distribution"
      ],
      "metadata": {
        "id": "VXBJ0-E7tHsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bengali nltk library import for FREQUENCY"
      ],
      "metadata": {
        "id": "6cFQdiYT0UpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # first-time setup\n",
        "\n",
        "text = \"আমি বাংলায় লিখছি। তুমি কি বাংলায় আসছ? আমি বাংলায় ভালো লিখতে চাই।\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(text, language='english')  # NLTK doesn't officially support Bengali, but it works for spaces & punctuation\n",
        "\n",
        "# Count frequencies\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Print\n",
        "for word, count in freq_dist.items():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMUv3A1Es6kk",
        "outputId": "240bc570-bd7a-45bf-ef02-379af7d79857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "আমি: 2\n",
            "বাংলায়: 3\n",
            "লিখছি।: 1\n",
            "তুমি: 1\n",
            "কি: 1\n",
            "আসছ: 1\n",
            "?: 1\n",
            "ভালো: 1\n",
            "লিখতে: 1\n",
            "চাই।: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bengali indiacnlp library import for FREQUENCY"
      ],
      "metadata": {
        "id": "0HGyGeXz0Rle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize, sentence_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "text = \"আমি বাংলায় লিখছি। তুমি কি বাংলায় আসছ? আমি বাংলায় ভালো লিখতে চাই। আমি বাংলায় লিখছি।\"\n",
        "\n",
        "# --- Sentence Tokenization ---\n",
        "sentences = sentence_tokenize.sentence_split(text, lang='bn')\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "# Count sentence frequency\n",
        "sentence_freq = Counter(sentences)\n",
        "print(\"\\nSentence Frequencies:\")\n",
        "for sentence, count in sentence_freq.items():\n",
        "    print(f\"{sentence}: {count}\")\n",
        "\n",
        "# --- Word Tokenization ---\n",
        "words = list(indic_tokenize.trivial_tokenize(text))\n",
        "print(\"\\nWords:\", words)\n",
        "\n",
        "# Count word frequency\n",
        "# defaultdict or Counter are easiest ways to count frequencies.\n",
        "wordfreq = Counter(words)\n",
        "print(\"\\nWord Frequencies:\")\n",
        "for word, count in wordfreq.items():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ucVPYulvoiI",
        "outputId": "71c5e0f5-bbda-4381-d9b4-79e0eb697d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['আমি বাংলায় লিখছি।', 'তুমি কি বাংলায় আসছ?', 'আমি বাংলায় ভালো লিখতে চাই।', 'আমি বাংলায় লিখছি।']\n",
            "\n",
            "Sentence Frequencies:\n",
            "আমি বাংলায় লিখছি।: 2\n",
            "তুমি কি বাংলায় আসছ?: 1\n",
            "আমি বাংলায় ভালো লিখতে চাই।: 1\n",
            "\n",
            "Words: ['আমি', 'বাংলায়', 'লিখছি', '।', 'তুমি', 'কি', 'বাংলায়', 'আসছ', '?', 'আমি', 'বাংলায়', 'ভালো', 'লিখতে', 'চাই', '।', 'আমি', 'বাংলায়', 'লিখছি', '।']\n",
            "\n",
            "Word Frequencies:\n",
            "আমি: 3\n",
            "বাংলায়: 4\n",
            "লিখছি: 2\n",
            "।: 3\n",
            "তুমি: 1\n",
            "কি: 1\n",
            "আসছ: 1\n",
            "?: 1\n",
            "ভালো: 1\n",
            "লিখতে: 1\n",
            "চাই: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER(Named entity recognition)"
      ],
      "metadata": {
        "id": "7kD7lM8bxm9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Download all the resource for NER\n",
        "# 1. word_tokenize → splits a sentence into individual words or punctuation marks (tokens).\n",
        "# 2. pos_tag → assigns Part-of-Speech (POS) tags to each token (like noun, verb, adjective).\n",
        "# 3. ne_chunk → identifies Named Entities (like persons, organizations, locations) based on POS-tagged\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "# 1. maxent_ne_chunker_tab → pre-trained model for recognizing named entities.\n",
        "# 2. words → dictionary of words needed for the chunker to work correctly\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDNlRypkxxj8",
        "outputId": "66ae214d-3ce0-42ac-8b5e-678f14e3207a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Java Compiler lab is conducted in room no 802\"\n",
        "\n",
        "# Tokenize and POS tag the sentence\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "#NNP → Proper noun, VBD → Verb, past tense, CD → Cardinal number\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "# Apply Named Entity Recognition\n",
        "entities = ne_chunk(tags)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2QV01N1x3a8",
        "outputId": "0cd00d06-5443-4b7d-b8b6-6e98033d0f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Java/NNP)\n",
            "  (ORGANIZATION Compiler/NNP)\n",
            "  lab/NN\n",
            "  is/VBZ\n",
            "  conducted/VBN\n",
            "  in/IN\n",
            "  room/NN\n",
            "  no/DT\n",
            "  802/CD)\n"
          ]
        }
      ]
    }
  ]
}